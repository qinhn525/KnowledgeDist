{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, json, pandas as pd\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers.data.data_collator import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/qhn/Codes/Models/Bert-base-chinese/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/qhn/Codes/Models/Bert-base-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /home/qhn/Codes/Models/Bert-base-chinese/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/qhn/Codes/Models/Bert-base-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/qhn/Codes/Models/Bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataloader(dataset_name, tokenizer, args):\n",
    "    collator = DataCollatorForLanguageModeling(tokenizer)\n",
    "    data_paths = {\n",
    "        \"train\": f\"/home/qhn/Codes/Projects/KnowledgeDist/Data/{dataset_name}/train.tsv\",\n",
    "        \"test\": f\"/home/qhn/Codes/Projects/KnowledgeDist/Data/{dataset_name}/test.tsv\"\n",
    "    }\n",
    "    label2index = json.load(open(f\"/home/qhn/Codes/Projects/KnowledgeDist/Data/{dataset_name}/label2index.json\", \"r\"))\n",
    "    train_data, test_data = {\"text\": [],\"labels\": []}, {\"text\": [],\"labels\": []}\n",
    "    train = pd.read_csv(data_paths['train'], sep=\"\\t\")\n",
    "    test  = pd.read_csv(data_paths['test'], sep=\"\\t\")\n",
    "    for idx, row in train.iterrows():\n",
    "        text, label = row['abstract'], label2index[row['discipline']]\n",
    "        train_data['text'].append(text)\n",
    "        train_data['labels'].append(label)\n",
    "    for idx, row in test.iterrows():\n",
    "        text, label = row['abstract'], label2index[row['discipline']]\n",
    "        test_data['text'].append(text)\n",
    "        test_data['labels'].append(label)\n",
    "    \n",
    "    train_dataset = Dataset.from_dict(train_data)\n",
    "    test_dataset = Dataset.from_dict(test_data)\n",
    "    \n",
    "    train_dataset = train_dataset.map(\n",
    "    lambda e: tokenizer(e['text'], truncation=True, padding='max_length', max_length=args[\"max_seq_length\"]), batched=True)\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=False,\n",
    "                             num_workers=0)\n",
    "    test_dataset = test_dataset.map(\n",
    "    lambda e: tokenizer(e['text'], truncation=True, padding='max_length', max_length=args[\"max_seq_length\"]), batched=True)\n",
    "    test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True, drop_last=False, collate_fn=collator, \n",
    "                             num_workers=0)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2793/2793 [00:00<00:00, 5239.98 examples/s]\n",
      "Map: 100%|██████████| 1197/1197 [00:00<00:00, 6703.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "args = {\"max_seq_length\": 256}\n",
    "train, test = load_dataloader(\"agriculture\", tokenizer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  4905,  -100,  -100,  -100,  2825,  3318,  -100,  -100,\n",
      "         -100,  -100,  -100,  5299,  -100,  -100,  1765,  -100,  -100,  -100,\n",
      "         -100,  -100,   119,   121,  -100,  -100,  -100,  -100,  1772,  -100,\n",
      "         -100,  -100,  -100,  -100,   129,  8595,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,   119,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  3946,  -100, 12929,  -100,  -100,\n",
      "         -100,  -100,  4288,  -100,  -100,  -100,  2108,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100, 11289,  -100,  -100,  8320,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  6574,   711,  -100,  5112,  -100,\n",
      "         -100,   117,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          117,  -100,  -100,  3300,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100])\n"
     ]
    }
   ],
   "source": [
    "for item in test:\n",
    "    print(item['labels'][0])\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KnowledgeDist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
